# Data Folder

This folder stores **local data artifacts** used by the project.

## Files

- `vozhsd_filtered_full.csv`  
  This is the main filtered dataset produced by `src/data_pipeline.py`.

  It is derived from the Hugging Face dataset `tarudesu/VOZ-HSD` and contains:

  - `text` — raw comment text (string)
  - `label` — binary label (0 = clean, 1 = hate/toxic)
  - `prob` — original weak-label probability from VOZ-HSD
  - `len_char` — number of characters in `text`
  - `len_word` — number of whitespace–split tokens in `text`

  Filtering rules applied in the pipeline:
  - Keep label = 1 if `prob >= MIN_PROB_POS` (default: 0.6)
  - Keep label = 0 if `prob >= MIN_PROB_NEG` (default: 0.9)
  - Remove comments with `len_word < MIN_LEN_WORD` (default: 3)

  The script **does not** store the original raw VOZ-HSD files here; it only saves the processed subset.

## How this folder is used

- `src/data_pipeline.py` writes `vozhsd_filtered_full.csv` here.
- `src/train_xgb.py` and `src/train_catboost.py` read this CSV as input.
- The CSV is typically **not** committed to Git if it is large; instead, it is regenerated by running the data pipeline.

If you are setting up the project on a new machine:

1. Install dependencies: `pip install -r requirements.txt`
2. Run: `python src/data_pipeline.py`
3. Confirm that `data/vozhsd_filtered_full.csv` was created successfully.
